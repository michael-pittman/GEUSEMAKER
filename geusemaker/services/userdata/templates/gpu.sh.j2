# ===================================
# GPU Validation and Setup
# ===================================

echo "GPU Validation and Setup"

{% if tier == "gpu" %}
# Verify NVIDIA GPU is present and drivers are loaded
echo "Checking for NVIDIA GPU..."

# Wait for nvidia-smi to become available (drivers may still be loading)
GPU_CHECK_ATTEMPTS=30
GPU_CHECK_DELAY=2
gpu_check_success=false

for attempt in $(seq 1 $GPU_CHECK_ATTEMPTS); do
    if nvidia-smi > /dev/null 2>&1; then
        gpu_check_success=true
        break
    fi
    echo "Waiting for NVIDIA drivers to load (attempt $attempt/$GPU_CHECK_ATTEMPTS)..."
    sleep $GPU_CHECK_DELAY
done

if [ "$gpu_check_success" = "false" ]; then
    echo "ERROR: NVIDIA GPU not detected after $((GPU_CHECK_ATTEMPTS * GPU_CHECK_DELAY)) seconds"
    echo "Ensure you're using a GPU instance type (g4dn, g5, p3, p4, p5, etc.)"
    echo "Also verify the AMI includes NVIDIA drivers (Deep Learning AMIs recommended)"
    exit 1
fi

# Display GPU information
echo "=== GPU Information ==="
nvidia-smi --query-gpu=gpu_name,driver_version,memory.total --format=csv,noheader,nounits

# Get GPU count
GPU_COUNT=$(nvidia-smi --list-gpus | wc -l)
echo "GPU Count: $GPU_COUNT"

# Set NVIDIA environment variables for optimal Ollama performance
# These will be available to all processes and Docker containers
cat >> /etc/environment <<'EOF'
# NVIDIA GPU Configuration for Ollama
NVIDIA_VISIBLE_DEVICES=all
NVIDIA_DRIVER_CAPABILITIES=compute,utility
# Keep Ollama models loaded in GPU memory indefinitely for faster inference
OLLAMA_KEEP_ALIVE=-1
# Allow Ollama to use all available GPUs
OLLAMA_NUM_GPU=$GPU_COUNT
EOF

echo "GPU validation completed successfully"
echo "GPU count: $GPU_COUNT"

{% else %}
echo "Non-GPU deployment - skipping GPU validation"
{% endif %}
