# Story 13.2: GPU Instance Support

## Status
Draft

## Story

**As a** user deploying GPU workloads,
**I want** GPU instance types to be properly supported,
**so that** I can run AI/ML workloads with NVIDIA GPUs.

## Acceptance Criteria

1. GPU instance types are validated
2. GPU-optimized AMIs are selected automatically
3. NVIDIA drivers are configured in UserData
4. Docker GPU runtime is configured
5. GPU availability is checked before deployment
6. GPU instance costs are estimated correctly
7. Service containers use GPU runtime when available
8. GPU health is validated post-deployment
9. Unit tests achieve 80%+ coverage

## Tasks / Subtasks

- [ ] Task 1: Define GPU Instance Types (AC: 1)
  - [ ] List supported GPU instance types
  - [ ] Define GPU specifications per type
  - [ ] Validate instance type input
  - [ ] Check regional availability

- [ ] Task 2: Select GPU-Optimized AMIs (AC: 2)
  - [ ] Identify Deep Learning AMIs
  - [ ] Query latest AMI per region
  - [ ] Fallback to standard AMI + drivers
  - [ ] Cache AMI lookups

- [ ] Task 3: Configure NVIDIA Drivers (AC: 3)
  - [ ] Add NVIDIA driver installation to UserData
  - [ ] Install CUDA toolkit
  - [ ] Configure driver persistence
  - [ ] Handle driver installation errors

- [ ] Task 4: Configure Docker GPU Runtime (AC: 4)
  - [ ] Install nvidia-container-toolkit
  - [ ] Configure Docker daemon for GPU
  - [ ] Set default runtime to nvidia
  - [ ] Verify GPU access from containers

- [ ] Task 5: Check GPU Availability (AC: 5)
  - [ ] Check instance type availability
  - [ ] Check capacity in region
  - [ ] Suggest alternative GPU types
  - [ ] Support spot GPU instances

- [ ] Task 6: Estimate GPU Costs (AC: 6)
  - [ ] Get GPU instance pricing
  - [ ] Calculate on-demand costs
  - [ ] Calculate spot savings
  - [ ] Include in cost estimation

- [ ] Task 7: Configure Service GPU Access (AC: 7)
  - [ ] Update Ollama to use GPU
  - [ ] Pass GPU device to containers
  - [ ] Set NVIDIA_VISIBLE_DEVICES
  - [ ] Configure CUDA memory limits

- [ ] Task 8: Validate GPU Health (AC: 8)
  - [ ] Run nvidia-smi check
  - [ ] Verify GPU accessible from Docker
  - [ ] Check Ollama GPU detection
  - [ ] Report GPU utilization

- [ ] Task 9: Create Unit Tests (AC: 9)
  - [ ] Test instance type validation
  - [ ] Test AMI selection
  - [ ] Test UserData generation
  - [ ] Test cost estimation
  - [ ] Achieve 80%+ coverage

## Dev Notes

### Architecture References
- See `docs/architecture/ARCHITECTURE.md` for EC2 service patterns
- GPU support extends EC2 service
- UserData script enhanced for GPU setup

### Supported GPU Instance Types
```python
GPU_INSTANCE_TYPES = {
    "g4dn.xlarge": {"gpu": "T4", "gpu_memory": "16GB", "vcpu": 4, "memory": "16GB"},
    "g4dn.2xlarge": {"gpu": "T4", "gpu_memory": "16GB", "vcpu": 8, "memory": "32GB"},
    "g5.xlarge": {"gpu": "A10G", "gpu_memory": "24GB", "vcpu": 4, "memory": "16GB"},
    "g5.2xlarge": {"gpu": "A10G", "gpu_memory": "24GB", "vcpu": 8, "memory": "32GB"},
    "p3.2xlarge": {"gpu": "V100", "gpu_memory": "16GB", "vcpu": 8, "memory": "61GB"},
}
```

### Deep Learning AMI
```python
DL_AMI_FILTER = {
    "name": "Deep Learning AMI (Ubuntu 22.04) *",
    "owner": "amazon"
}
```

### UserData GPU Section
```bash
# GPU Setup
nvidia-smi  # Verify driver
# Install nvidia-container-toolkit
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
# Configure Docker for GPU
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
```

### Docker Compose GPU Config
```yaml
services:
  ollama:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
```

## Testing

### Unit Tests
- Test GPU instance type validation
- Test AMI selection logic
- Test UserData generation with GPU
- Test cost estimation for GPU types
- Mock nvidia-smi checks

### Integration Tests
- Deploy to GPU instance (if available)
- Verify GPU accessible
- Test Ollama GPU inference
- Validate health checks

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-21 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record
_To be populated by Dev Agent_

## QA Results
_To be populated by QA Agent_
